{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laden-athens",
   "metadata": {},
   "source": [
    "## Set things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contemporary-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor  # not sure I'm going to use this\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_SOURCE = \"C:/Users/sbranchett/Data/WiFi_data/WiFiAccessPoint.csv\"\n",
    "CLEAN_DATA_SOURCE = \"CleanedWiFiAccessPoint.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-passport",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3597531\n"
     ]
    }
   ],
   "source": [
    "def load_wifi_data(path):\n",
    "    \"\"\"\n",
    "    Read WiFi clientCount .csv file and sum the clientCounts over building and timestamp, bucketed to 5 minutes\n",
    "    \n",
    "    Input: filepath the .csv file\n",
    "    Output: Dataframe with \"building\", \"time_bucket\", \"clientCount\"\n",
    "    Columns \"building\" generated from \"locationHierarchy\", and \"time_bucket\" generated from \"timestamp\"\n",
    "    \"\"\"\n",
    "    all_data = pd.read_csv(path, delimiter=\",\")\n",
    "    all_data = all_data.rename(columns=lambda x: x.strip())  # get rid of extra spaces in column names\n",
    "\n",
    "    sum_clientCounts = all_data[\"clientCount\"].sum()\n",
    "    \n",
    "    # extract building from 'locationHierarchy' string and deal with unknown buildings\n",
    "    all_data[\"building\"] = all_data[\"locationHierarchy\"].str.split(\" > \",expand=True)[1]\n",
    "    all_data[\"building\"] = all_data[\"building\"].fillna(\"Unknown\")\n",
    "\n",
    "    # convert timestamp from epoch milliseconds to 5 minute buckets (1000 milliseconds * 60 seconds * 5 minutes)\n",
    "    all_data[\"time_bucket\"] = all_data[\"timestamp\"].apply(lambda d: 300000*int(d/300000))\n",
    "    \n",
    "    # keep only the interesting columns\n",
    "    all_data = all_data[[\"time_bucket\", \"building\", \"clientCount\"]]\n",
    "    \n",
    "    # sum data over buildings and time buckets\n",
    "    all_data = all_data.groupby([\"time_bucket\", \"building\"]).sum()  # sort on time first so that new data keeps sequence\n",
    "    \n",
    "    assert sum_clientCounts == all_data[\"clientCount\"].sum()  # check that no clientCounts went missing\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# if possible read in the cleaned data, otherwise clean the raw data and save\n",
    "if os.path.isfile(CLEAN_DATA_SOURCE):\n",
    "    all_data = pd.read_csv(CLEAN_DATA_SOURCE, delimiter=\",\")\n",
    "else:\n",
    "    all_data = load_wifi_data(RAW_DATA_SOURCE)\n",
    "    all_data.to_csv(CLEAN_DATA_SOURCE)\n",
    "\n",
    "print(all_data[\"clientCount\"].sum())  # should be 3597531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separated-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time_bucket               building  clientCount\n",
      "0      1620991800000      03-Science Center           34\n",
      "1      1620991800000             05-TNW-BIO            1\n",
      "2      1620991800000             08-BK-City            2\n",
      "3      1620991800000   19-Studuitzendbureau           13\n",
      "4      1620991800000                20-Aula           32\n",
      "...              ...                    ...          ...\n",
      "78161  1621608600000                 64-HSL           10\n",
      "78162  1621608600000          66-Fellowship          126\n",
      "78163  1621608600000  Katalyse Labaratorium            8\n",
      "78164  1621608600000                Unknown           32\n",
      "78165  1621608600000           VLL-LAB(TNO)           11\n",
      "\n",
      "[78166 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.reset_index().drop(\"index\", axis=1)  # get rid of multiindex for StratifiedShuffleSplit\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-sussex",
   "metadata": {},
   "source": [
    "## Separate Test Set stratified over buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "desperate-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time_bucket                           building  clientCount\n",
      "11688  1621083900000                    37-Sportcentrum           15\n",
      "61351  1621476000000                         36-ESP-Lab            1\n",
      "55780  1621431900000                      66-Fellowship          101\n",
      "26158  1621198200000                          32-OCP-IO           12\n",
      "34879  1621266900000                             64-HSL           16\n",
      "...              ...                                ...          ...\n",
      "50892  1621393500000                            28- WNI            4\n",
      "33498  1621256100000  36-EWI LB_K t/m 3 & HB_K  t/m 2e           247\n",
      "4160   1621024500000                      35-Drebbelweg            2\n",
      "56082  1621434300000                          63-Simona            8\n",
      "55786  1621432200000                         08-BK-City          670\n",
      "\n",
      "[62532 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(all_data, all_data[\"building\"]):\n",
    "    strat_train_set = all_data.loc[train_index]\n",
    "    strat_test_set = all_data.loc[test_index]\n",
    "\n",
    "print(strat_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-fraud",
   "metadata": {},
   "source": [
    "## Create Work days and Student days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prescribed-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_hols = (datetime.date(year=2021, month=5, day=5),\n",
    "            datetime.date(year=2021, month=5, day=13),\n",
    "            datetime.date(year=2021, month=5, day=14),\n",
    "            datetime.date(year=2021, month=5, day=24)\n",
    "           )  # National Holidays and Collective Free days\n",
    "\n",
    "def roster(date):\n",
    "    # categorise the days of the academic year at TU Delft between 1 May 2021 and 29 August 2021\n",
    "    \n",
    "    if (date > datetime.date(year=2021, month=5, day=16)) and \\\n",
    "       (date < datetime.date(year=2021, month=5, day=22)):\n",
    "        categorie = \"Exam_BSc\"\n",
    "    elif (date > datetime.date(year=2021, month=6, day=15)) and \\\n",
    "         (date < datetime.date(year=2021, month=6, day=19)):\n",
    "        categorie = \"Study_mixed\"\n",
    "    elif (date > datetime.date(year=2021, month=6, day=22)) and \\\n",
    "         (date < datetime.date(year=2021, month=6, day=26)):\n",
    "        categorie = \"Exam_mixed\"\n",
    "    elif (date > datetime.date(year=2021, month=6, day=29)) and \\\n",
    "         (date < datetime.date(year=2021, month=7, day=3)):\n",
    "        categorie = \"Exam\"\n",
    "    elif (date > datetime.date(year=2021, month=8, day=8)) and \\\n",
    "         (date < datetime.date(year=2021, month=8, day=14)):\n",
    "        categorie = \"Exam\"\n",
    "    elif (date > datetime.date(year=2021, month=7, day=4)):\n",
    "        categorie = \"Free\"\n",
    "    else:\n",
    "        categorie = \"Learn\"\n",
    "    return categorie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-mathematics",
   "metadata": {},
   "source": [
    "## Separate predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "different-giving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time_bucket                           building\n",
      "11688  1621083900000                    37-Sportcentrum\n",
      "61351  1621476000000                         36-ESP-Lab\n",
      "55780  1621431900000                      66-Fellowship\n",
      "26158  1621198200000                          32-OCP-IO\n",
      "34879  1621266900000                             64-HSL\n",
      "...              ...                                ...\n",
      "50892  1621393500000                            28- WNI\n",
      "33498  1621256100000  36-EWI LB_K t/m 3 & HB_K  t/m 2e \n",
      "4160   1621024500000                      35-Drebbelweg\n",
      "56082  1621434300000                          63-Simona\n",
      "55786  1621432200000                         08-BK-City\n",
      "\n",
      "[62532 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "crowd = strat_train_set.drop(\"clientCount\", axis=1)\n",
    "crowd_labels = strat_train_set[\"clientCount\"].copy()\n",
    "print(crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shaped-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert crowd.isnull().values.any() == False  # check there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-contractor",
   "metadata": {},
   "source": [
    "## Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developing-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which extra attributes to add to the data\n",
    "# want to treat these as hyperparameters for grid search\n",
    "add_day_of_week=True\n",
    "add_time_of_day=True\n",
    "add_uni_hols=True\n",
    "add_weekend=True\n",
    "add_academic_yr=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animated-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up categories for OneHotEncoder.\n",
    "# This is needed later for the transformation of small sets that do not contain all the categories.\n",
    "\n",
    "buildings = list(set(all_data[\"building\"].values))\n",
    "rosters = [\"Exam_BSc\", \"Study_mixed\", \"Exam_mixed\", \"Exam\", \"Free\", \"Learn\"]  # get a list of all roster categories, SEB could be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "touched-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_day_of_week=True, add_time_of_day=True, add_uni_hols=True, add_weekend=True, add_academic_yr=True):\n",
    "        self.add_day_of_week = add_day_of_week\n",
    "        self.add_time_of_day = add_time_of_day\n",
    "        self.add_uni_hols = add_uni_hols\n",
    "        self.add_weekend = add_weekend\n",
    "        self.add_academic_yr = add_academic_yr\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_trans = X.copy()\n",
    "        if self.add_day_of_week:\n",
    "            X_trans[\"day_of_week\"] = X[\"time_bucket\"].apply(lambda d: datetime.datetime.fromtimestamp(d/1000).weekday())\n",
    "        if self.add_time_of_day:\n",
    "            X_trans[\"time_of_day\"] = X[\"time_bucket\"].apply(lambda d: int(d%86400000))\n",
    "        if self.add_uni_hols:\n",
    "            X_trans[\"hols\"] = X[\"time_bucket\"].apply(lambda d: 1. if datetime.datetime.fromtimestamp(d/1000).date() in uni_hols else 0.)\n",
    "        if self.add_weekend:\n",
    "            X_trans[\"weekend\"] = X[\"time_bucket\"].apply(lambda d: 1. if datetime.datetime.fromtimestamp(d/1000).weekday() > 4 else 0.)\n",
    "        if self.add_academic_yr:\n",
    "            X_trans[\"academic_yr\"] = X[\"time_bucket\"].apply(lambda d: roster(datetime.datetime.fromtimestamp(d/1000).date()))\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crazy-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved the column transformation to it's own class so that it could be dependent on attributes\n",
    "# Pity I have to specifiy the same attributes to both classes\n",
    "class CombinedColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_day_of_week=True, add_time_of_day=True, add_uni_hols=True, add_weekend=True, add_academic_yr=True,\n",
    "                 buildings=None, rosters=None):\n",
    "        self.add_day_of_week = add_day_of_week\n",
    "        self.add_time_of_day = add_time_of_day\n",
    "        self.add_uni_hols = add_uni_hols\n",
    "        self.add_weekend = add_weekend\n",
    "        self.add_academic_yr = add_academic_yr\n",
    "        self.buildings = buildings\n",
    "        self.rosters = rosters\n",
    "        self.num_attribs = [\"time_bucket\"]\n",
    "        self.cat_attribs = [\"building\"]\n",
    "        self.all_categories = [buildings]\n",
    "        if self.add_day_of_week:\n",
    "            self.num_attribs.extend([\"day_of_week\"])\n",
    "        if self.add_time_of_day:\n",
    "            self.num_attribs.extend([\"time_of_day\"])\n",
    "        if self.add_uni_hols:\n",
    "            self.num_attribs.extend([\"hols\"])\n",
    "        if self.add_weekend:\n",
    "            self.num_attribs.extend([\"weekend\"])\n",
    "        if self.add_academic_yr:\n",
    "            self.all_categories.append(rosters)\n",
    "            self.cat_attribs.extend([\"academic_yr\"])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        coltrans = ColumnTransformer([(\"num\", StandardScaler(), self.num_attribs),\n",
    "                                      (\"cat\", OneHotEncoder(categories=self.all_categories), self.cat_attribs)\n",
    "                                     ])\n",
    "        X_trans = coltrans.fit_transform(X)\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-beauty",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# combine the pipelines based on column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "italian-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    ('add_attributes', CombinedAttributesAdder(add_day_of_week, add_time_of_day, add_uni_hols, add_weekend, add_academic_yr)),\n",
    "    ('trans_columns', CombinedColumnTransformer(add_day_of_week, add_time_of_day, add_uni_hols, add_weekend, add_academic_yr,\n",
    "                                                buildings=buildings, rosters=rosters)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "external-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedAttributesAdder()\n"
     ]
    }
   ],
   "source": [
    "print(full_pipeline.named_steps['add_attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consecutive-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedColumnTransformer(buildings=['28- WNI', '26-Bouwcampus',\n",
      "                                     '32a- Learninglab', '36-EWI-HB',\n",
      "                                     '63-Simona', 'VLL-LAB(TNO)', '20-Aula',\n",
      "                                     '36-ESP-Lab', '61-Vliegtuighal',\n",
      "                                     '66-Fellowship', '45-LSL',\n",
      "                                     '03-Science Center', '23-CITG',\n",
      "                                     '30-IKC_ISD-FMVG', '35-Drebbelweg',\n",
      "                                     '60-LMS', '31-TBM',\n",
      "                                     '36-EWI LB_K t/m 3 & HB_K  t/m 2e ',\n",
      "                                     '25-GreenVillage', '33-Pulse', '62-LR',\n",
      "                                     '34-3ME', '19-Studuitzendbureau',\n",
      "                                     '37-Sportcentrum', '08-BK-City', 'Unknown',\n",
      "                                     '21-BTUD', '22-TNW-TN', '05-TNW-BIO',\n",
      "                                     '38-Cultureel Centrum', ...],\n",
      "                          rosters=['Exam_BSc', 'Study_mixed', 'Exam_mixed',\n",
      "                                   'Exam', 'Free', 'Learn'])\n"
     ]
    }
   ],
   "source": [
    "print(full_pipeline.named_steps['trans_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polish-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_pipeline.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "minute-campaign",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.21669404  0.9978856   0.16027025 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.98681612 -0.00801088 -1.45222465 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.73898445 -0.51095912  0.25726243 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.55050812  0.49493736  1.25143222 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.75247189 -0.51095912  0.3542546  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.74067038 -0.51095912  0.26938645 ...  0.          0.\n",
      "   0.        ]]\n",
      "(62532, 49)\n"
     ]
    }
   ],
   "source": [
    "crowd_prepared = full_pipeline.fit_transform(crowd)  # this gives a sparse matrix, sometimes!\n",
    "print(crowd_prepared.toarray())\n",
    "#print(crowd_prepared)\n",
    "print(crowd_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-prayer",
   "metadata": {},
   "source": [
    "## Select and Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "labeled-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "signed-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(crowd_prepared, crowd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tested-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crowd_attributes = CombinedAttributesAdder(add_day_of_week, add_time_of_day, add_uni_hols, add_weekend, add_academic_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "engaged-village",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crowd_attributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2cb44bbed1b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msome_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrowd_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msome_data_extra_attribs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrowd_attributes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_data_extra_attribs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crowd_attributes' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare a small data set to test\n",
    "some_data = crowd.iloc[:5]\n",
    "some_labels = crowd_labels[:5]\n",
    "\n",
    "some_data_extra_attribs = crowd_attributes.transform(some_data)\n",
    "print(some_data_extra_attribs)\n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data_extra_attribs)  # this gives a sparse matrix!\n",
    "print(some_data_prepared)\n",
    "print(some_data_prepared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the predictions\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_predictions = lin_reg.predict(crowd_prepared)\n",
    "lin_mse = mean_squared_error(crowd_labels, crowd_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, crowd_prepared, crowd_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(crowd_prepared, crowd_labels)\n",
    "\n",
    "crowd_predictions = tree_reg.predict(crowd_prepared)\n",
    "tree_mse = mean_squared_error(crowd_labels, crowd_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(tree_reg, crowd_prepared, crowd_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the predictions\n",
    "print(\"Predictions:\", tree_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-trademark",
   "metadata": {},
   "source": [
    "## Let's take a look at how are predictions are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_stamp = all_data[\"time_bucket\"].min()\n",
    "last_time_stamp = all_data[\"time_bucket\"].max()\n",
    "\n",
    "a_building = \"20-Aula\"\n",
    "a_building_data = strat_train_set.loc[strat_train_set[\"building\"] == a_building]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = list(range(first_time_stamp, last_time_stamp + 300000, 300000))\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test[\"time_bucket\"] = times\n",
    "test[\"building\"] = a_building\n",
    "\n",
    "test_extra_attribs = crowd_attributes.transform(test)\n",
    "test_prepared = full_pipeline.transform(test_extra_attribs)\n",
    "\n",
    "lin_preds = lin_reg.predict(test_prepared)\n",
    "tree_preds = tree_reg.predict(test_prepared)\n",
    "xes = test[\"time_bucket\"]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title(a_building)\n",
    "plt.axis([first_time_stamp, last_time_stamp, -100, 200])\n",
    "plt.plot(a_building_data[\"time_bucket\"], a_building_data[\"clientCount\"], 'bo', times, lin_preds, 'g--', times, tree_preds, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-scenario",
   "metadata": {},
   "source": [
    "## See what happens when we try to predict the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_days = 1\n",
    "latest_time_stamp = last_time_stamp + extra_days*86400000 + 300000\n",
    "times = list(range(first_time_stamp, latest_time_stamp, 300000))\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test[\"time_bucket\"] = times\n",
    "test[\"building\"] = a_building\n",
    "test_extra_attribs = crowd_attributes.transform(test)\n",
    "test_prepared = full_pipeline.transform(test_extra_attribs)\n",
    "lin_preds = lin_reg.predict(test_prepared)\n",
    "tree_preds = tree_reg.predict(test_prepared)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title(a_building)\n",
    "plt.axis([first_time_stamp, latest_time_stamp, -100, 200])\n",
    "plt.plot(a_building_data[\"time_bucket\"], a_building_data[\"clientCount\"], 'bo', times, lin_preds, 'g--', times, tree_preds, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-estonia",
   "metadata": {},
   "source": [
    "Save what we have so far:\n",
    "```\n",
    "from joblib import dump\n",
    "\n",
    "dump(lin_reg, \"lin_reg.joblib\")\n",
    "dump(tree_reg, \"tree_reg.joblib\")\n",
    "dump(forest_reg, \"forest_reg.joblib\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-electron",
   "metadata": {},
   "source": [
    "Still want to do:\n",
    "\n",
    "* add grid search\n",
    "* possibly add time from lunch as an attribute\n",
    "* see what happens with 4 weeks of data\n",
    "* write blog\n",
    "* illustrate blog with nice graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])\n",
    "\n",
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'add_day_of_week': [True, False], 'add_time_of_day': [True, False], 'add_uni_hols': [True, False],\n",
    "     'add_weekend': [True, False], 'add_academic_yr': [True, False]}\n",
    "]\n",
    "\n",
    "# lin_reg = LinearRegression()\n",
    "\n",
    "grid_search = GridSearchCV(lin_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search.fit(crowd_prepared, crowd_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-float",
   "metadata": {},
   "source": [
    "See also: https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbook",
   "language": "python",
   "name": "mlbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
